{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --q\n",
        "!pip install langchain --q\n",
        "!pip install unstructured --q\n",
        "!pip install unstructured libmagic python-magic python-magic-bin --q\n",
        "!pip install faiss -cpu\n",
        "!pip install sentance-transformers\n",
        "!pip install gradio\n",
        "!pip install streamlit\n",
        "!pip install faiss-gpu\n",
        "!pip install pickle\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "E-gpfN0-4mDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdF8mLMm4TFY",
        "outputId": "067f01c5-19d6-4673-ef86-81f3d7baf4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:24: UserWarning: Importing OpenAI from langchain root module is no longer supported.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import time\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']='****'"
      ],
      "metadata": {
        "id": "ihUko1nH4q1u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI(temperature=0.7,max_tokens=500)\n",
        "loader=UnstructuredURLLoader(urls=[\n",
        "    \"https://economictimes.indiatimes.com/industry/banking/finance/crypto-drama-unfolds-binance-bnb-connects-goodbye-bitcoin-btcs-silent-play-and-pomerdoge-pomds-rising-star/articleshow/103142105.cms\",\n",
        "    \"https://en.m.wikipedia.org/wiki/Jubilant_FoodWorks#:~:text=Jubilant%20FoodWorks%20Limited%20is%20an,for%20Dunkin'%20Donuts%20in%20India\"\n",
        "])\n",
        "data=loader.load()"
      ],
      "metadata": {
        "id": "1XJ31mvnURcm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting into chunks\n",
        "r_splitter=RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\",\"\\n\",\".\",\" \"], #diff between recursive text split and text split is that we can include a variety of separators and not just one.\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\"\"\"\n",
        "First text sep w \\n\\n and then if size still more than allowed chunk size then split again with \\n and then . and so on\n",
        "Merge also happens in this step where the chunks that are too small it gets merged so they overlap a little.\n",
        "\"\"\"\n",
        "docs=r_splitter.split_documents(data)\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tAyHAbvYpYW",
        "outputId": "c3deec10-f741-4737-d416-e7f8affa29ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Embeddings\n",
        "embeddings=OpenAIEmbeddings()\n",
        "vectorstore_openai=FAISS.from_documents(docs,embeddings)"
      ],
      "metadata": {
        "id": "6fdA5PsTYqdk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save FAISS index to pickle file\n",
        "#Store vector in local\n",
        "file_path=\"faiss_store_openai.pkl\"\n",
        "with open(file_path,\"wb\") as f:\n",
        "  pickle.dump(vectorstore_openai,f)\n"
      ],
      "metadata": {
        "id": "g5QOfuQ_Y-6c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def querys(query):\n",
        "  if query:\n",
        "    if os.path.isfile(file_path):\n",
        "      with open(file_path,\"rb\") as f:\n",
        "        vectorstore=pickle.load(f)\n",
        "        chain=RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "        result=chain({\"question\":query},return_only_outputs=True)\n",
        "        #print(result['answer'])#op\n",
        "        return result['answer'],result['sources']"
      ],
      "metadata": {
        "id": "DbNXGt4rZs-c"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface=gr.Interface(\n",
        "    fn=querys,\n",
        "    inputs=gr.Textbox(label=\"Enter query below\"),\n",
        "    outputs=[gr.Textbox(label=\"Answer\"),gr.Textbox(label=\"Source\")],\n",
        "    title=\"Article Research Tool\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "NQfX_0VBdA0X"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "gA2Nm6npeZbj",
        "outputId": "403c634e-5752-4e42-9297-8764c302f4cb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://7c7b4e640cf8b9feff.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7c7b4e640cf8b9feff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D73BQTN5ebpL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}